{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70487a4-e98c-45e5-8bc8-200e239fc57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 17:26:10.076857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, BatchNormalization, \n",
    "    Input, Attention, Flatten, Permute, Multiply, Lambda\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1285f85-989c-44f6-8c52-87c559683c56",
   "metadata": {},
   "source": [
    "# Configurations and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69db6d7f-8442-4599-b109-b211ab24a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_READY_DATA_DIR = \"Dataset/ai_ready_data\"  # Preprocessed data directory\n",
    "MODEL_SAVE_PATH = \"lstm_location_predictor.keras\"  # Trained Keras model\n",
    "ONNX_MODEL_SAVE_PATH = \"lstm_location_predictor.onnx\" # ONNX model\n",
    "\n",
    "# LSTM Model Hyperparameters\n",
    "LSTM_UNITS_1 = 128       # Number of units in the first LSTM layer\n",
    "LSTM_UNITS_2 = 64        # Number of units in the second LSTM layer\n",
    "DENSE_UNITS_1 = 128      # Number of units in the first Dense layer\n",
    "DENSE_UNITS_2 = 64       # Number of units in the second Dense layer\n",
    "DROPOUT_RATE = 0.3       # Dropout rate for regularization\n",
    "LEARNING_RATE = 0.001    # Learning rate for the Adam optimizer\n",
    "BATCH_SIZE = 64          # Number of samples per gradient update\n",
    "EPOCHS = 50              # Maximum number of epochs for training (EarlyStopping will monitor)\n",
    "USE_ATTENTION = False    # Whether to use an attention mechanism (as per proposal \"optional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae748175-7680-4fb0-bc90-5312c8f741cc",
   "metadata": {},
   "source": [
    "# Load the preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31305c1-3d46-4698-ba03-55afc775e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    try:\n",
    "        X_train = np.load(os.path.join(data_dir, \"X_train.npy\"), allow_pickle=True)\n",
    "        y_train = np.load(os.path.join(data_dir, \"y_train.npy\"), allow_pickle=True)\n",
    "        \n",
    "        # Handle potentially empty validation/test sets if data split was small\n",
    "        X_val_path = os.path.join(data_dir, \"X_val.npy\")\n",
    "        X_val = np.load(X_val_path, allow_pickle=True) if os.path.exists(X_val_path) and os.path.getsize(X_val_path) > 0 else np.array([])\n",
    "        \n",
    "        y_val_path = os.path.join(data_dir, \"y_val.npy\")\n",
    "        y_val = np.load(y_val_path, allow_pickle=True) if os.path.exists(y_val_path) and os.path.getsize(y_val_path) > 0 else np.array([])\n",
    "        \n",
    "        X_test_path = os.path.join(data_dir, \"X_test.npy\")\n",
    "        X_test = np.load(X_test_path, allow_pickle=True) if os.path.exists(X_test_path) and os.path.getsize(X_test_path) > 0 else np.array([])\n",
    "        \n",
    "        y_test_path = os.path.join(data_dir, \"y_test.npy\")\n",
    "        y_test = np.load(y_test_path, allow_pickle=True) if os.path.exists(y_test_path) and os.path.getsize(y_test_path) > 0 else np.array([])\n",
    "\n",
    "        with open(os.path.join(data_dir, \"grid_id_to_index.json\"), \"r\") as f:\n",
    "            grid_id_to_index = json.load(f)\n",
    "\n",
    "        print(\"Data loaded successfully.\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        if X_val.size > 0:\n",
    "            print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        if X_test.size > 0:\n",
    "            print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test, grid_id_to_index\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Data file not found. {e}\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced1362-e497-44a9-845b-eb6fc3fdb39c",
   "metadata": {},
   "source": [
    "# Define Model Archtecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f948e3be-2f5c-4034-a7c3-6fa989bfac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sequence_length, num_features, num_classes):\n",
    "    print(\"Building LSTM model...\")\n",
    "    \n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # First LSTM layer\n",
    "    x = LSTM(LSTM_UNITS_1, return_sequences=(LSTM_UNITS_2 > 0 or USE_ATTENTION))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # Optional Attention Layer (simple self-attention on LSTM output)\n",
    "    if USE_ATTENTION:\n",
    "        if not (LSTM_UNITS_2 > 0): # If no second LSTM, LSTM1 output needs to be sequence for attention\n",
    "            pass\n",
    "        \n",
    "        # A simple attention mechanism\n",
    "        attention_probs = Dense(sequence_length, activation='softmax', name='attention_vec')(x)\n",
    "        \n",
    "        # Simpler attention: weighted sum of LSTM outputs\n",
    "        # This requires LSTM_UNITS_1 to return_sequences=True\n",
    "        attention_mul = Multiply()([x, Permute((2,1))(Dense(num_features, activation='softmax')(x))]) # Element-wise multiplication after permuting dense output\n",
    "        attention_mul = Lambda(lambda xin: tf.keras.backend.sum(xin, axis=1))(attention_mul) # Sum over time steps\n",
    "        \n",
    "        # If using tf.keras.layers.Attention directly:\n",
    "        # attention_result = Attention()([x, x])\n",
    "        # x = Flatten()(attention_result) \n",
    "\n",
    "        x = attention_mul # Use the weighted sum as input to the next layer\n",
    "        \n",
    "    # Optional second LSTM layer\n",
    "    if LSTM_UNITS_2 > 0:\n",
    "        if USE_ATTENTION:\n",
    "            print(\"Warning: Using Attention with a second LSTM layer requires careful architecture. Current setup assumes Attention is followed by Dense layers.\")\n",
    "        else: # No attention, standard stacked LSTM\n",
    "            x = LSTM(LSTM_UNITS_2, return_sequences=False)(x) # return_sequences=False for the last LSTM before Dense\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(DENSE_UNITS_1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    if DENSE_UNITS_2 > 0:\n",
    "        x = Dense(DENSE_UNITS_2, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', # Use this for integer targets\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Model built and compiled successfully.\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbc621-e761-4633-8d25-ddd8224e5899",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39cfa646-4fb4-4de3-9aa1-7256b8f11183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Trains the LSTM model.\"\"\"\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    callbacks = []\n",
    "    if X_val.size > 0 and y_val.size > 0:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "        callbacks.append(early_stopping)\n",
    "    else: # No validation set, train for full epochs or use training loss for early stopping (less ideal)\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "        callbacks.append(early_stopping)\n",
    "        print(\"Warning: No validation data provided. Early stopping will monitor training loss.\")\n",
    "\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss' if X_val.size > 0 else 'loss', \n",
    "                                       save_best_only=True, verbose=1)\n",
    "    callbacks.append(model_checkpoint)\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data=(X_val, y_val) if X_val.size > 0 and y_val.size > 0 else None,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=1)\n",
    "    \n",
    "    print(\"Model training completed.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c6808-2a26-49c3-9e39-7d7a7558c4b8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ca0506-d963-4b5c-9e8c-925e616a635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, num_classes):\n",
    "    \"\"\"Evaluates the trained model on the test set.\"\"\"\n",
    "    if X_test.size == 0 or y_test.size == 0:\n",
    "        print(\"No test data to evaluate.\")\n",
    "        return\n",
    "\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Classification report and confusion matrix can be very large for many classes\n",
    "    if num_classes <= 50: # Arbitrary threshold to avoid excessive output\n",
    "        try:\n",
    "            y_pred_probs = model.predict(X_test)\n",
    "            y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred_classes, zero_division=0))\n",
    "            # print(\"\\nConfusion Matrix (sample):\") # Full matrix too large\n",
    "            # print(confusion_matrix(y_test, y_pred_classes)) # Might be too large to print meaningfully\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate classification report/confusion matrix: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping classification report due to large number of classes ({num_classes}).\")\n",
    "        print(\"Consider implementing Top-N accuracy or other relevant metrics for high-cardinality classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5f19c-cba1-4245-bbcc-44d64244853a",
   "metadata": {},
   "source": [
    "# Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2fd770-a72c-4493-9f19-7d0b7d52bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plots training and validation loss and accuracy.\"\"\"\n",
    "    if history is None:\n",
    "        print(\"No training history to plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    print(\"Training history plot saved as training_history.png\")\n",
    "    # plt.show() # Uncomment if running in an environment that supports interactive plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03d249-dadd-474c-90ea-8ab3aff2fd2b",
   "metadata": {},
   "source": [
    "# Save Model in ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b84ad94-bd8e-40fd-b226-bd49ca647d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_onnx(keras_model_path, onnx_model_path, sequence_length, num_features):\n",
    "    \"\"\"Converts and saves the Keras model to ONNX format.\"\"\"\n",
    "    try:\n",
    "        import tf2onnx\n",
    "        import tensorflow as tf # Ensure tf is available for tf.keras.models.load_model\n",
    "\n",
    "        print(f\"Loading Keras model from {keras_model_path} for ONNX conversion...\")\n",
    "        model = tf.keras.models.load_model(keras_model_path) # Load the best saved Keras model\n",
    "\n",
    "        # Define the input signature for the ONNX model\n",
    "        # Batch size is dynamic (None), sequence length and num_features are fixed.\n",
    "        spec = (tf.TensorSpec((None, sequence_length, num_features), tf.float32, name=\"input\"),)\n",
    "        \n",
    "        print(f\"Converting Keras model to ONNX format (saving to {onnx_model_path})...\")\n",
    "        model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13) # opset can be adjusted\n",
    "        \n",
    "        with open(onnx_model_path, \"wb\") as f:\n",
    "            f.write(model_proto.SerializeToString())\n",
    "        print(f\"Model successfully converted and saved to {onnx_model_path}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"tf2onnx library not found. Skipping ONNX conversion.\")\n",
    "        print(\"To install: pip install tf2onnx\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during ONNX conversion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ab234-4bce-40a7-b3ed-d8bf6cd3dc32",
   "metadata": {},
   "source": [
    "# The main execution segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f03b80-8f4f-4dae-a86c-a8530cebc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while loading data: Object arrays cannot be loaded when allow_pickle=False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, grid_id_to_idx \u001b[38;5;241m=\u001b[39m load_data(AI_READY_DATA_DIR)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X_train_scaled\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training data loaded. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, grid_id_to_idx = load_data(AI_READY_DATA_DIR)\n",
    "\n",
    "    if X_train_scaled.size == 0:\n",
    "        print(\"No training data loaded. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Determine model parameters\n",
    "    sequence_len = X_train_scaled.shape[1]\n",
    "    num_feat = X_train_scaled.shape[2]\n",
    "    num_classes_output = len(grid_id_to_idx)\n",
    "    \n",
    "    print(f\"Sequence Length: {sequence_len}, Number of Features: {num_feat}, Number of Classes: {num_classes_output}\")\n",
    "\n",
    "    # Build the model\n",
    "    lstm_model = build_model(sequence_len, num_feat, num_classes_output)\n",
    "    \n",
    "    # Train the model\n",
    "    training_history = train_model(lstm_model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_history(training_history)\n",
    "\n",
    "    # Load the best model saved by ModelCheckpoint for evaluation and ONNX conversion\n",
    "    print(f\"Loading best saved Keras model from {MODEL_SAVE_PATH} for final evaluation and ONNX export...\")\n",
    "    try:\n",
    "        best_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the saved Keras model: {e}\")\n",
    "        print(\"Proceeding with the model from memory if available, but ONNX export might fail or use last state.\")\n",
    "        best_model = lstm_model # Fallback to model in memory\n",
    "\n",
    "    # Evaluate the best model\n",
    "    if X_test_scaled.size > 0 and y_test.size > 0:\n",
    "        evaluate_model(best_model, X_test_scaled, y_test, num_classes_output)\n",
    "    else:\n",
    "        print(\"Skipping evaluation on test set as it's empty.\")\n",
    "        \n",
    "    # Save in ONNX format (optional)\n",
    "    save_model_onnx(MODEL_SAVE_PATH, ONNX_MODEL_SAVE_PATH, sequence_len, num_feat)\n",
    "\n",
    "    print(\"LSTM Model Script Execution Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
