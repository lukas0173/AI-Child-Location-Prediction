{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382158b0-068e-46aa-ae39-357784f6d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split # Though chronological split is preferred\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add3c7d-6c4a-4e44-9254-d7c8d6cbccd7",
   "metadata": {},
   "source": [
    "# Configuration & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51f9977-1f7e-46bf-86f1-ea4caabe1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CRS = \"EPSG:32648\"\n",
    "\n",
    "# LSTM Sequence Generation\n",
    "SEQUENCE_LENGTH = 10 # Length of input sequence for LSTM\n",
    "PREDICTION_HORIZON = 1 # How many steps ahead to predict (e.g., 1 means next step)\n",
    "\n",
    "# --- File Paths (assuming they are in the same directory or provide full paths) ---\n",
    "GRID_FEATURES_PATH = \"./GIS/danang_grid_with_features.parquet\"\n",
    "MOVEMENT_DATA_PATH = \"./HistoricalMovement/danang_movement_processed_decrypted.parquet\"\n",
    "HISTORICAL_WEATHER_PATH = \"./WeatherForecast/danang_historical_weather_oikolab_processed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada01bd-2b4d-42c4-9b71-9a7b171c9ec6",
   "metadata": {},
   "source": [
    "# Load briefly Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ef1b33-16dd-46f6-855f-94815d2a5842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded grid data: (212350, 238)\n",
      "Successfully loaded movement data: (6857, 10)\n",
      "Successfully loaded historical weather data: (10521, 25)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    grid_gdf = gpd.read_parquet(GRID_FEATURES_PATH)\n",
    "    print(f\"Successfully loaded grid data: {grid_gdf.shape}\")\n",
    "    # Ensure 'grid_id' is the index for easier joining later if it's unique and suitable\n",
    "    if 'grid_id' in grid_gdf.columns and grid_gdf['grid_id'].is_unique:\n",
    "        grid_gdf = grid_gdf.set_index('grid_id')\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Grid data file not found at {GRID_FEATURES_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load grid data: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    movement_df = pd.read_parquet(MOVEMENT_DATA_PATH)\n",
    "    print(f\"Successfully loaded movement data: {movement_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Movement data file not found at {MOVEMENT_DATA_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load movement data: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    historical_weather_df = pd.read_csv(HISTORICAL_WEATHER_PATH)\n",
    "    print(f\"Successfully loaded historical weather data: {historical_weather_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Historical weather data file not found at {HISTORICAL_WEATHER_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load historical weather data: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141a598-db0e-4a48-8af9-b4ab186e1082",
   "metadata": {},
   "source": [
    "# Further Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3572210-9451-4e1f-ade8-0ae2a6cfbde6",
   "metadata": {},
   "source": [
    "### GIS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93898124-d1e3-4681-9a12-530800383caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GIS data 'road_density' BEFORE normalization (sample & describe):\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: road_density, dtype: float64\n",
      "count    212350.000000\n",
      "mean          0.003638\n",
      "std           0.011472\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.121717\n",
      "Name: road_density, dtype: float64\n",
      "Normalized GIS columns in grid_gdf: ['building_count_apartments', 'building_count_bridge', 'building_count_cathedral', 'building_count_church', 'building_count_civic', 'building_count_college', 'building_count_commercial', 'building_count_construction', 'building_count_dam', 'building_count_dormitory', 'building_count_grandstand', 'building_count_hangar', 'building_count_hospital', 'building_count_hotel', 'building_count_house', 'building_count_hut', 'building_count_industrial', 'building_count_navigationaid', 'building_count_no', 'building_count_office', 'building_count_public', 'building_count_residential', 'building_count_restaurant', 'building_count_retail', 'building_count_roof', 'building_count_ruins', 'building_count_school', 'building_count_sports_centre', 'building_count_stadium', 'building_count_temple', 'building_count_terrace', 'building_count_tower', 'building_count_train_station', 'building_count_transportation', 'building_count_university', 'building_count_warehouse', 'building_count_yes', 'poi_count_vinmart+', 'poi_count_alcohol', 'poi_count_amusement_arcade', 'poi_count_apartment', 'poi_count_art', 'poi_count_artwork', 'poi_count_atm', 'poi_count_attraction', 'poi_count_baby_goods', 'poi_count_bag', 'poi_count_bakery', 'poi_count_bank', 'poi_count_bar', 'poi_count_beauty', 'poi_count_bed', 'poi_count_bench', 'poi_count_beverages', 'poi_count_bicycle', 'poi_count_bicycle_parking', 'poi_count_bicycle_rental', 'poi_count_bicycle_repair_station', 'poi_count_biergarten', 'poi_count_bleachers', 'poi_count_bookmaker', 'poi_count_books', 'poi_count_bureau_de_change', 'poi_count_bus_station', 'poi_count_butcher', 'poi_count_cafe', 'poi_count_camp_site', 'poi_count_car', 'poi_count_car_parts', 'poi_count_car_rental', 'poi_count_car_repair', 'poi_count_car_wash', 'poi_count_casino', 'poi_count_chalet', 'poi_count_charging_station', 'poi_count_chocolate', 'poi_count_cinema', 'poi_count_clock', 'poi_count_clothes', 'poi_count_coffee', 'poi_count_community_centre', 'poi_count_computer', 'poi_count_confectionery', 'poi_count_convenience', 'poi_count_copyshop', 'poi_count_cosmetics', 'poi_count_dentist', 'poi_count_department_store', 'poi_count_doctors', 'poi_count_drinking_water', 'poi_count_dry_cleaning', 'poi_count_electronics', 'poi_count_events_venue', 'poi_count_fast_food', 'poi_count_ferry_terminal', 'poi_count_fitness_centre', 'poi_count_fitness_station', 'poi_count_florist', 'poi_count_food_court', 'poi_count_fountain', 'poi_count_fuel', 'poi_count_furniture', 'poi_count_gallery', 'poi_count_garden', 'poi_count_gift', 'poi_count_grave_yard', 'poi_count_greengrocer', 'poi_count_guest_house', 'poi_count_hairdresser', 'poi_count_hospital', 'poi_count_hostel', 'poi_count_hotel', 'poi_count_ice_cream', 'poi_count_information', 'poi_count_interior_decoration', 'poi_count_internet_cafe', 'poi_count_jewelry', 'poi_count_karaoke_box', 'poi_count_kindergarten', 'poi_count_laundry', 'poi_count_library', 'poi_count_marketplace', 'poi_count_massage', 'poi_count_mobile_phone', 'poi_count_motel', 'poi_count_motorcycle', 'poi_count_motorcycle_parking', 'poi_count_motorcycle_rental', 'poi_count_motorcycle_repair', 'poi_count_museum', 'poi_count_musical_instrument', 'poi_count_nightclub', 'poi_count_online_gaming', 'poi_count_optician', 'poi_count_outdoor', 'poi_count_park', 'poi_count_parking', 'poi_count_pastry', 'poi_count_pawnbroker', 'poi_count_pet', 'poi_count_pharmacy', 'poi_count_photo', 'poi_count_photo_booth', 'poi_count_picnic_site', 'poi_count_pitch', 'poi_count_place_of_worship', 'poi_count_platform', 'poi_count_playground', 'poi_count_police', 'poi_count_post_box', 'poi_count_post_office', 'poi_count_pub', 'poi_count_radiotechnics', 'poi_count_reception_desk', 'poi_count_resort', 'poi_count_restaurant', 'poi_count_sanitation', 'poi_count_sauna', 'poi_count_school', 'poi_count_scooter_rental', 'poi_count_seafood', 'poi_count_second_hand', 'poi_count_shelter', 'poi_count_shoes', 'poi_count_shower', 'poi_count_smartshop', 'poi_count_smoking_area', 'poi_count_sports', 'poi_count_sports_centre', 'poi_count_sports_hall', 'poi_count_stadium', 'poi_count_station', 'poi_count_stationery', 'poi_count_stop_position', 'poi_count_storage_rental', 'poi_count_supermarket', 'poi_count_swimming_pool', 'poi_count_tattoo', 'poi_count_taxi', 'poi_count_tea', 'poi_count_theatre', 'poi_count_theme_park', 'poi_count_ticket', 'poi_count_toilets', 'poi_count_townhall', 'poi_count_track', 'poi_count_travel_agency', 'poi_count_tyres', 'poi_count_university', 'poi_count_variety_store', 'poi_count_vending_machine', 'poi_count_veterinary', 'poi_count_viewpoint', 'poi_count_waste_basket', 'poi_count_waste_disposal', 'poi_count_watches', 'poi_count_water_park', 'poi_count_water_sports', 'poi_count_wilderness_hut', 'poi_count_yes', 'poi_count_zoo', 'total_poi_count', 'road_length_m', 'road_density']\n",
      "GIS data 'road_density' AFTER normalization (sample & describe):\n",
      "grid_id\n",
      "cell_0    0.0\n",
      "cell_1    0.0\n",
      "cell_2    0.0\n",
      "cell_3    0.0\n",
      "cell_4    0.0\n",
      "Name: road_density, dtype: float64\n",
      "count    212350.000000\n",
      "mean          0.029888\n",
      "std           0.094253\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: road_density, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'dominant_building_type_none' and other one-hot encoded columns exist if 'none' was a category\n",
    "if 'btype_none' not in grid_gdf.columns and 'dominant_building_type' in movement_with_grid.columns: \n",
    "     # This check implies that 'none' might have been a category for dominant_building_type\n",
    "     print(\"Available btype columns:\", [col for col in grid_gdf.columns if col.startswith('btype_')])\n",
    "\n",
    "grid_gdf_original_for_stats = gpd.read_parquet(GRID_FEATURES_PATH) # Load original again for before/after stats\n",
    "# Normalize numerical GIS features (road_density, poi_counts, building_counts)\n",
    "gis_cols_to_normalize = [col for col in grid_gdf.columns if 'count' in col or 'density' in col or 'length' in col]\n",
    "# Filter out non-numeric columns just in case\n",
    "gis_numeric_cols_to_normalize = [col for col in gis_cols_to_normalize if pd.api.types.is_numeric_dtype(grid_gdf[col])]\n",
    "\n",
    "if gis_numeric_cols_to_normalize:\n",
    "    if 'road_density' in grid_gdf_original_for_stats.columns:\n",
    "        print(\"\\nGIS data 'road_density' BEFORE normalization (sample & describe):\")\n",
    "        print(grid_gdf_original_for_stats['road_density'].head())\n",
    "        print(grid_gdf_original_for_stats['road_density'].describe())\n",
    "        \n",
    "    scaler_gis = MinMaxScaler()\n",
    "    grid_gdf[gis_numeric_cols_to_normalize] = scaler_gis.fit_transform(grid_gdf[gis_numeric_cols_to_normalize])\n",
    "    print(f\"Normalized GIS columns in grid_gdf: {gis_numeric_cols_to_normalize}\")\n",
    "\n",
    "    if 'road_density' in grid_gdf.columns:\n",
    "        print(\"GIS data 'road_density' AFTER normalization (sample & describe):\")\n",
    "        print(grid_gdf['road_density'].head())\n",
    "        print(grid_gdf['road_density'].describe())\n",
    "else:\n",
    "    print(\"No GIS columns found to normalize in grid_gdf.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c300d4-794c-4295-a7ca-88a9102edd79",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699daa98-9890-499d-b69d-55857c24ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized weather columns: ['temp_c', 'relative_humidity', 'wind_speed_mps', 'cloud_cover', 'precip_mm']\n"
     ]
    }
   ],
   "source": [
    "# Convert historical weather timestamp to datetime and ensure UTC\n",
    "if not pd.api.types.is_datetime64_any_dtype(historical_weather_df['timestamp_utc']):\n",
    "    historical_weather_df['timestamp_utc'] = pd.to_datetime(historical_weather_df['timestamp_utc'], utc=True)\n",
    "else:\n",
    "    historical_weather_df['timestamp_utc'] = historical_weather_df['timestamp_utc'].dt.tz_localize('UTC')\n",
    "\n",
    "# Select relevant columns and set timestamp_utc as index for easy lookup\n",
    "weather_df_processed = historical_weather_df.set_index('timestamp_utc')\n",
    "\n",
    "# Normalize numerical weather features (example, more can be added)\n",
    "weather_cols_to_normalize = ['temp_c', 'relative_humidity', 'wind_speed_mps', 'cloud_cover', 'precip_mm']\n",
    "# Check which columns are actually present\n",
    "weather_cols_present = [col for col in weather_cols_to_normalize if col in weather_df_processed.columns]\n",
    "\n",
    "if weather_cols_present:\n",
    "    scaler_weather = MinMaxScaler()\n",
    "    weather_df_processed[weather_cols_present] = scaler_weather.fit_transform(weather_df_processed[weather_cols_present])\n",
    "    print(f\"Normalized weather columns: {weather_cols_present}\")\n",
    "else:\n",
    "    print(\"No weather columns found to normalize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1127c-8405-4694-86df-9e2bb898cbeb",
   "metadata": {},
   "source": [
    "### Movement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa30c81-7ab5-4e4a-8ab3-a1998c8650ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Movement Data...\n",
      "Original movement data shape: (6857, 10)\n",
      "Movement data sample BEFORE dropping NaNs:\n",
      "               TimestampUTC   Latitude   Longitude\n",
      "0 2024-11-20 11:05:23+00:00  16.074451  108.152405\n",
      "1 2024-11-20 11:05:23+00:00  16.074451  108.152405\n",
      "2 2024-11-20 11:09:48+00:00  16.074177  108.152565\n",
      "3 2024-11-20 11:05:23+00:00  16.074451  108.152405\n",
      "4 2024-11-20 06:38:47+00:00  16.074172  108.152865\n",
      "\n",
      "Movement data shape AFTER dropping NaNs in Lat/Lon/Timestamp: (6857, 10)\n",
      "Movement data sample AFTER dropping NaNs:\n",
      "               TimestampUTC   Latitude   Longitude\n",
      "0 2024-11-20 11:05:23+00:00  16.074451  108.152405\n",
      "1 2024-11-20 11:05:23+00:00  16.074451  108.152405\n",
      "2 2024-11-20 11:09:48+00:00  16.074177  108.152565\n",
      "3 2024-11-20 11:05:23+00:00  16.074451  108.152405\n",
      "4 2024-11-20 06:38:47+00:00  16.074172  108.152865\n",
      "\n",
      "Performing spatial join between movement data and grid cells...\n",
      "Movement data shape after assigning grid_id: 6857\n",
      "Movement data shape after dropping non-gridded points: (6857, 13)\n",
      "Movement data sample WITH grid_id:\n",
      "               TimestampUTC   Latitude   Longitude     grid_id\n",
      "0 2024-11-20 11:05:23+00:00  16.074451  108.152405  cell_78463\n",
      "1 2024-11-20 11:05:23+00:00  16.074451  108.152405  cell_78463\n",
      "2 2024-11-20 11:09:48+00:00  16.074177  108.152565  cell_78463\n",
      "3 2024-11-20 11:05:23+00:00  16.074451  108.152405  cell_78463\n",
      "4 2024-11-20 06:38:47+00:00  16.074172  108.152865  cell_78463\n",
      "\n",
      "Movement data sample with cyclical time features:\n",
      "               TimestampUTC  hour_sin_mov  hour_cos_mov  day_of_week_sin_mov  \\\n",
      "0 2024-11-20 11:05:23+00:00      0.258819 -9.659258e-01             0.974928   \n",
      "1 2024-11-20 11:05:23+00:00      0.258819 -9.659258e-01             0.974928   \n",
      "2 2024-11-20 11:09:48+00:00      0.258819 -9.659258e-01             0.974928   \n",
      "3 2024-11-20 11:05:23+00:00      0.258819 -9.659258e-01             0.974928   \n",
      "4 2024-11-20 06:38:47+00:00      1.000000  6.123234e-17             0.974928   \n",
      "\n",
      "   day_of_week_cos_mov  \n",
      "0            -0.222521  \n",
      "1            -0.222521  \n",
      "2            -0.222521  \n",
      "3            -0.222521  \n",
      "4            -0.222521  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Original movement data shape: {movement_df.shape}\")\n",
    "print(\"Movement data sample BEFORE dropping NaNs:\")\n",
    "print(movement_df[['TimestampUTC', 'Latitude', 'Longitude']].head())\n",
    "\n",
    "# Corrected TimestampUTC handling for movement_df\n",
    "if 'TimestampUTC' in movement_df.columns:\n",
    "    if not pd.api.types.is_datetime64_any_dtype(movement_df['TimestampUTC']):\n",
    "        movement_df['TimestampUTC'] = pd.to_datetime(movement_df['TimestampUTC'], errors='coerce', utc=False) # Convert to naive first\n",
    "    \n",
    "    # Now, check if it's timezone aware after potential conversion\n",
    "    if movement_df['TimestampUTC'].dt.tz is None: # If naive\n",
    "        movement_df['TimestampUTC'] = movement_df['TimestampUTC'].dt.tz_localize('UTC')\n",
    "    else: # If aware\n",
    "        movement_df['TimestampUTC'] = movement_df['TimestampUTC'].dt.tz_convert('UTC')\n",
    "else:\n",
    "    print(\"ERROR: 'TimestampUTC' column not found in movement_df.\")\n",
    "    # Handle this case, perhaps by exiting or creating a dummy column if appropriate\n",
    "    exit()\n",
    "\n",
    "\n",
    "movement_df.dropna(subset=['Latitude', 'Longitude', 'TimestampUTC'], inplace=True)\n",
    "print(f\"\\nMovement data shape AFTER dropping NaNs in Lat/Lon/Timestamp: {movement_df.shape}\")\n",
    "print(\"Movement data sample AFTER dropping NaNs:\")\n",
    "print(movement_df[['TimestampUTC', 'Latitude', 'Longitude']].head())\n",
    "\n",
    "\n",
    "try:\n",
    "    movement_gdf = gpd.GeoDataFrame(\n",
    "        movement_df,\n",
    "        geometry=gpd.points_from_xy(movement_df.Longitude, movement_df.Latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    movement_gdf = movement_gdf.to_crs(TARGET_CRS)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not convert movement data to GeoDataFrame or reproject: {e}\")\n",
    "    exit()\n",
    "\n",
    "if grid_gdf.index.name == 'grid_id':\n",
    "    grid_gdf_for_join = grid_gdf.reset_index()\n",
    "else:\n",
    "    grid_gdf_for_join = grid_gdf\n",
    "    \n",
    "if 'geometry' not in grid_gdf_for_join.columns:\n",
    "     print(\"ERROR: 'geometry' column not found in grid_gdf_for_join. Check grid data loading.\")\n",
    "     exit()\n",
    "if 'grid_id' not in grid_gdf_for_join.columns:\n",
    "     print(\"ERROR: 'grid_id' column not found in grid_gdf_for_join. Check grid data loading.\")\n",
    "     exit()\n",
    "\n",
    "print(\"\\nPerforming spatial join between movement data and grid cells...\")\n",
    "movement_with_grid = gpd.sjoin(movement_gdf, grid_gdf_for_join[['grid_id', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "original_gridded_count = len(movement_with_grid)\n",
    "movement_with_grid.dropna(subset=['grid_id'], inplace=True)\n",
    "print(f\"Movement data shape after assigning grid_id: {original_gridded_count}\")\n",
    "print(f\"Movement data shape after dropping non-gridded points: {movement_with_grid.shape}\")\n",
    "print(\"Movement data sample WITH grid_id:\")\n",
    "print(movement_with_grid[['TimestampUTC', 'Latitude', 'Longitude', 'grid_id']].head())\n",
    "\n",
    "if 'index_right' in movement_with_grid.columns:\n",
    "    movement_with_grid.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "dt_col_movement = movement_with_grid['TimestampUTC']\n",
    "movement_with_grid['hour_sin_mov'] = np.sin(2 * np.pi * dt_col_movement.dt.hour / 24)\n",
    "movement_with_grid['hour_cos_mov'] = np.cos(2 * np.pi * dt_col_movement.dt.hour / 24)\n",
    "movement_with_grid['day_of_week_sin_mov'] = np.sin(2 * np.pi * dt_col_movement.dt.dayofweek / 7)\n",
    "movement_with_grid['day_of_week_cos_mov'] = np.cos(2 * np.pi * dt_col_movement.dt.dayofweek / 7)\n",
    "movement_with_grid['month_sin_mov'] = np.sin(2 * np.pi * (dt_col_movement.dt.month -1) / 12)\n",
    "movement_with_grid['month_cos_mov'] = np.cos(2 * np.pi * (dt_col_movement.dt.month -1) / 12)\n",
    "print(\"\\nMovement data sample with cyclical time features:\")\n",
    "print(movement_with_grid[['TimestampUTC', 'hour_sin_mov', 'hour_cos_mov', 'day_of_week_sin_mov', 'day_of_week_cos_mov']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2e90e-1382-4a91-a3f3-263d9569b38f",
   "metadata": {},
   "source": [
    "# Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e85ce5e-b339-497b-adf2-68808d2b9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (6857, 279)\n",
      "\n",
      "NaN counts in merged_df AFTER weather merge (first 10 columns):\n",
      "LocationID               0\n",
      "DeviceID                 0\n",
      "TimestampUTC             0\n",
      "Latitude                 0\n",
      "Longitude                0\n",
      "Confidence               0\n",
      "Description              0\n",
      "StatusCode               0\n",
      "DBDatePublishedUTC    6857\n",
      "EncryptedPayloadDB       0\n",
      "dtype: int64\n",
      "NaN counts in merged_df AFTER weather merge (weather columns):\n",
      "coordinates_lat_lon    0\n",
      "model_name             0\n",
      "model_elevation_m      0\n",
      "utc_offset_hrs         0\n",
      "temp_c                 0\n",
      "relative_humidity      0\n",
      "wind_speed_mps         0\n",
      "wind_deg               0\n",
      "wind_gust_mps          0\n",
      "cloud_cover            0\n",
      "precip_mm              0\n",
      "hour_of_day            0\n",
      "day_of_week            0\n",
      "day_of_year            0\n",
      "month_of_year          0\n",
      "year                   0\n",
      "hour_sin               0\n",
      "hour_cos               0\n",
      "day_of_week_sin        0\n",
      "day_of_week_cos        0\n",
      "month_sin              0\n",
      "month_cos              0\n",
      "day_of_year_sin        0\n",
      "day_of_year_cos        0\n",
      "dtype: int64\n",
      "DBDatePublishedUTC column is all NaN. Dropping it before final dropna.\n",
      "Merged data shape after final dropna: (6857, 278)\n",
      "\n",
      "Sample of Merged Data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>DeviceID</th>\n",
       "      <th>TimestampUTC</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Description</th>\n",
       "      <th>StatusCode</th>\n",
       "      <th>EncryptedPayloadDB</th>\n",
       "      <th>geometry</th>\n",
       "      <th>...</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>year</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_week_sin</th>\n",
       "      <th>day_of_week_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>395</td>\n",
       "      <td>afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=</td>\n",
       "      <td>2024-11-14 00:05:26+00:00</td>\n",
       "      <td>16.074298</td>\n",
       "      <td>108.152192</td>\n",
       "      <td>175.0</td>\n",
       "      <td>found</td>\n",
       "      <td>0</td>\n",
       "      <td>LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...</td>\n",
       "      <td>POINT (837278.062 1779724.608)</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.726225</td>\n",
       "      <td>0.687457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2687</td>\n",
       "      <td>afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=</td>\n",
       "      <td>2024-11-14 00:05:26+00:00</td>\n",
       "      <td>16.074298</td>\n",
       "      <td>108.152192</td>\n",
       "      <td>175.0</td>\n",
       "      <td>found</td>\n",
       "      <td>0</td>\n",
       "      <td>LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...</td>\n",
       "      <td>POINT (837278.062 1779724.608)</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.726225</td>\n",
       "      <td>0.687457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>204</td>\n",
       "      <td>afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=</td>\n",
       "      <td>2024-11-14 00:05:26+00:00</td>\n",
       "      <td>16.074298</td>\n",
       "      <td>108.152192</td>\n",
       "      <td>175.0</td>\n",
       "      <td>found</td>\n",
       "      <td>0</td>\n",
       "      <td>LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...</td>\n",
       "      <td>POINT (837278.062 1779724.608)</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.726225</td>\n",
       "      <td>0.687457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>586</td>\n",
       "      <td>afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=</td>\n",
       "      <td>2024-11-14 00:05:26+00:00</td>\n",
       "      <td>16.074298</td>\n",
       "      <td>108.152192</td>\n",
       "      <td>175.0</td>\n",
       "      <td>found</td>\n",
       "      <td>0</td>\n",
       "      <td>LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...</td>\n",
       "      <td>POINT (837278.062 1779724.608)</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.726225</td>\n",
       "      <td>0.687457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>968</td>\n",
       "      <td>afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=</td>\n",
       "      <td>2024-11-14 00:05:26+00:00</td>\n",
       "      <td>16.074298</td>\n",
       "      <td>108.152192</td>\n",
       "      <td>175.0</td>\n",
       "      <td>found</td>\n",
       "      <td>0</td>\n",
       "      <td>LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...</td>\n",
       "      <td>POINT (837278.062 1779724.608)</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.726225</td>\n",
       "      <td>0.687457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LocationID                                      DeviceID  \\\n",
       "383          395  afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=   \n",
       "2675        2687  afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=   \n",
       "192          204  afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=   \n",
       "574          586  afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=   \n",
       "956          968  afirx1LlNk5vh7BnbGukU+L8o9E3pHhd/uogNOdmdv8=   \n",
       "\n",
       "                  TimestampUTC   Latitude   Longitude  Confidence Description  \\\n",
       "383  2024-11-14 00:05:26+00:00  16.074298  108.152192       175.0       found   \n",
       "2675 2024-11-14 00:05:26+00:00  16.074298  108.152192       175.0       found   \n",
       "192  2024-11-14 00:05:26+00:00  16.074298  108.152192       175.0       found   \n",
       "574  2024-11-14 00:05:26+00:00  16.074298  108.152192       175.0       found   \n",
       "956  2024-11-14 00:05:26+00:00  16.074298  108.152192       175.0       found   \n",
       "\n",
       "      StatusCode                                 EncryptedPayloadDB  \\\n",
       "383            0  LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...   \n",
       "2675           0  LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...   \n",
       "192            0  LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...   \n",
       "574            0  LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...   \n",
       "956            0  LOV2RgABBIWu4zxJRX7u/P68FRyE8fAlXW7CXFbwlmEcY0...   \n",
       "\n",
       "                            geometry  ... month_of_year  year  hour_sin  \\\n",
       "383   POINT (837278.062 1779724.608)  ...            11  2024       0.0   \n",
       "2675  POINT (837278.062 1779724.608)  ...            11  2024       0.0   \n",
       "192   POINT (837278.062 1779724.608)  ...            11  2024       0.0   \n",
       "574   POINT (837278.062 1779724.608)  ...            11  2024       0.0   \n",
       "956   POINT (837278.062 1779724.608)  ...            11  2024       0.0   \n",
       "\n",
       "      hour_cos  day_of_week_sin  day_of_week_cos  month_sin  month_cos  \\\n",
       "383        1.0         0.433884        -0.900969  -0.866025        0.5   \n",
       "2675       1.0         0.433884        -0.900969  -0.866025        0.5   \n",
       "192        1.0         0.433884        -0.900969  -0.866025        0.5   \n",
       "574        1.0         0.433884        -0.900969  -0.866025        0.5   \n",
       "956        1.0         0.433884        -0.900969  -0.866025        0.5   \n",
       "\n",
       "      day_of_year_sin  day_of_year_cos  \n",
       "383         -0.726225         0.687457  \n",
       "2675        -0.726225         0.687457  \n",
       "192         -0.726225         0.687457  \n",
       "574         -0.726225         0.687457  \n",
       "956         -0.726225         0.687457  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if grid_gdf.index.name == 'grid_id':\n",
    "    # If grid_gdf is indexed by 'grid_id', we use join. Ensure movement_with_grid also has 'grid_id' as a regular column for joining.\n",
    "    if 'grid_id' not in movement_with_grid.columns:\n",
    "        print(\"ERROR: 'grid_id' not in movement_with_grid columns for joining.\")\n",
    "        exit()\n",
    "    merged_df = movement_with_grid.join(grid_gdf.drop(columns=['geometry'], errors='ignore'), on='grid_id', how='left')\n",
    "else: # If grid_gdf is not indexed by 'grid_id', use merge.\n",
    "    merged_df = pd.merge(movement_with_grid, grid_gdf.drop(columns=['geometry'], errors='ignore'), on='grid_id', how='left')\n",
    "\n",
    "\n",
    "merged_df['timestamp_round_hour_utc'] = merged_df['TimestampUTC'].dt.round('h')\n",
    "\n",
    "if not pd.api.types.is_datetime64_any_dtype(weather_df_processed.index):\n",
    "     weather_df_processed.index = pd.to_datetime(weather_df_processed.index, utc=True)\n",
    "elif weather_df_processed.index.tz is None:\n",
    "    weather_df_processed.index = weather_df_processed.index.tz_localize('UTC')\n",
    "\n",
    "# Ensure columns to merge on are sorted for merge_asof\n",
    "merged_df = merged_df.sort_values('timestamp_round_hour_utc')\n",
    "weather_df_processed = weather_df_processed.sort_index()\n",
    "\n",
    "\n",
    "merged_df = pd.merge_asof(\n",
    "    merged_df,\n",
    "    weather_df_processed,\n",
    "    left_on='timestamp_round_hour_utc',\n",
    "    right_index=True,\n",
    "    direction='nearest',\n",
    "    suffixes=('', '_weather')\n",
    ")\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "# It's crucial to inspect NaNs after merging, especially from weather data if time alignment is imperfect\n",
    "print(\"\\nNaN counts in merged_df AFTER weather merge (first 10 columns):\")\n",
    "print(merged_df.isnull().sum().head(10))\n",
    "print(\"NaN counts in merged_df AFTER weather merge (weather columns):\")\n",
    "weather_related_cols_in_merged = [col for col in merged_df.columns if col in weather_df_processed.columns or '_weather' in col]\n",
    "print(merged_df[weather_related_cols_in_merged].isnull().sum())\n",
    "\n",
    "if 'DBDatePublishedUTC' in merged_df.columns and merged_df['DBDatePublishedUTC'].isna().all():\n",
    "    print(\"DBDatePublishedUTC column is all NaN. Dropping it before final dropna.\")\n",
    "    merged_df.drop(columns=['DBDatePublishedUTC'], inplace=True)\n",
    "\n",
    "\n",
    "merged_df.dropna(inplace=True) # Consider a more targeted dropna based on critical columns\n",
    "print(f\"Merged data shape after final dropna: {merged_df.shape}\")\n",
    "print(\"\\nSample of Merged Data (first 5 rows):\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "merged_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
