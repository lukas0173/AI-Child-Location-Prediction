{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70487a4-e98c-45e5-8bc8-200e239fc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, BatchNormalization, \n",
    "    Input, Attention, Flatten, Permute, Multiply, Lambda\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1285f85-989c-44f6-8c52-87c559683c56",
   "metadata": {},
   "source": [
    "# Configurations and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69db6d7f-8442-4599-b109-b211ab24a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_READY_DATA_DIR = \"ai_ready_data\"  # Preprocessed data directory\n",
    "MODEL_SAVE_PATH = \"lstm_location_predictor.keras\"  # Trained Keras model\n",
    "ONNX_MODEL_SAVE_PATH = \"lstm_location_predictor.onnx\" # ONNX model\n",
    "\n",
    "# LSTM Model Hyperparameters\n",
    "LSTM_UNITS_1 = 128       # Number of units in the first LSTM layer\n",
    "LSTM_UNITS_2 = 64        # Number of units in the second LSTM layer\n",
    "DENSE_UNITS_1 = 128      # Number of units in the first Dense layer\n",
    "DENSE_UNITS_2 = 64       # Number of units in the second Dense layer\n",
    "DROPOUT_RATE = 0.3       # Dropout rate for regularization\n",
    "LEARNING_RATE = 0.001    # Learning rate for the Adam optimizer\n",
    "BATCH_SIZE = 64          # Number of samples per gradient update\n",
    "EPOCHS = 50              # Maximum number of epochs for training (EarlyStopping will monitor)\n",
    "USE_ATTENTION = False    # Whether to use an attention mechanism (as per proposal \"optional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae748175-7680-4fb0-bc90-5312c8f741cc",
   "metadata": {},
   "source": [
    "# Load the preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31305c1-3d46-4698-ba03-55afc775e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    try:\n",
    "        X_train = np.load(os.path.join(data_dir, \"X_train.npy\"))\n",
    "        y_train = np.load(os.path.join(data_dir, \"y_train.npy\"))\n",
    "        \n",
    "        # Handle potentially empty validation/test sets if data split was small\n",
    "        X_val_path = os.path.join(data_dir, \"X_val.npy\")\n",
    "        X_val = np.load(X_val_path) if os.path.exists(X_val_path) and os.path.getsize(X_val_path) > 0 else np.array([])\n",
    "        \n",
    "        y_val_path = os.path.join(data_dir, \"y_val.npy\")\n",
    "        y_val = np.load(y_val_path) if os.path.exists(y_val_path) and os.path.getsize(y_val_path) > 0 else np.array([])\n",
    "        \n",
    "        X_test_path = os.path.join(data_dir, \"X_test.npy\")\n",
    "        X_test = np.load(X_test_path) if os.path.exists(X_test_path) and os.path.getsize(X_test_path) > 0 else np.array([])\n",
    "        \n",
    "        y_test_path = os.path.join(data_dir, \"y_test.npy\")\n",
    "        y_test = np.load(y_test_path) if os.path.exists(y_test_path) and os.path.getsize(y_test_path) > 0 else np.array([])\n",
    "\n",
    "        with open(os.path.join(data_dir, \"grid_id_to_index.json\"), \"r\") as f:\n",
    "            grid_id_to_index = json.load(f)\n",
    "\n",
    "        print(\"Data loaded successfully.\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        if X_val.size > 0:\n",
    "            print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        if X_test.size > 0:\n",
    "            print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test, grid_id_to_index\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Data file not found. {e}\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced1362-e497-44a9-845b-eb6fc3fdb39c",
   "metadata": {},
   "source": [
    "# Define Model Archtecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948e3be-2f5c-4034-a7c3-6fa989bfac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sequence_length, num_features, num_classes):\n",
    "    print(\"Building LSTM model...\")\n",
    "    \n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # First LSTM layer\n",
    "    x = LSTM(LSTM_UNITS_1, return_sequences=(LSTM_UNITS_2 > 0 or USE_ATTENTION))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # Optional Attention Layer (simple self-attention on LSTM output)\n",
    "    if USE_ATTENTION:\n",
    "        if not (LSTM_UNITS_2 > 0): # If no second LSTM, LSTM1 output needs to be sequence for attention\n",
    "            pass\n",
    "        \n",
    "        # A simple attention mechanism\n",
    "        attention_probs = Dense(sequence_length, activation='softmax', name='attention_vec')(x)\n",
    "        # The Permute and Multiply layers are common for applying attention weights.\n",
    "        # However, Keras's Attention layer is simpler if it fits the need.\n",
    "        # Using tf.keras.layers.Attention for a more standard approach:\n",
    "        # query_value_attention_seq = Attention()([x, x]) # Self-attention\n",
    "        \n",
    "        # Simpler attention: weighted sum of LSTM outputs\n",
    "        # This requires LSTM_UNITS_1 to return_sequences=True\n",
    "        attention_mul = Multiply()([x, Permute((2,1))(Dense(num_features, activation='softmax')(x))]) # Element-wise multiplication after permuting dense output\n",
    "        attention_mul = Lambda(lambda xin: tf.keras.backend.sum(xin, axis=1))(attention_mul) # Sum over time steps\n",
    "        \n",
    "        # If using tf.keras.layers.Attention directly:\n",
    "        # attention_result = Attention()([x, x]) # query, value\n",
    "        # x = Flatten()(attention_result) # Flatten if attention output is still sequential\n",
    "\n",
    "        x = attention_mul # Use the weighted sum as input to the next layer\n",
    "        # Note: This is a basic attention. More complex mechanisms exist.\n",
    "        # For this project, keeping it optional and relatively simple.\n",
    "        # If LSTM_UNITS_2 > 0, this attention output (which is now 1D) needs to be handled.\n",
    "        # The current setup assumes attention output is flattened/summarized before dense layers\n",
    "        # or that the next LSTM layer can handle its shape.\n",
    "        # If attention is used and followed by another LSTM, LSTM1 must return_sequences=True.\n",
    "        # The attention layer output would then be processed.\n",
    "        # For now, if attention is used, it will be followed by Dense layers, so we need to flatten if 'x' is still a sequence.\n",
    "        # The current simple attention_mul already reduces dimensionality.\n",
    "\n",
    "    # Optional second LSTM layer\n",
    "    if LSTM_UNITS_2 > 0:\n",
    "        # If USE_ATTENTION is True, 'x' might be shaped (batch_size, LSTM_UNITS_1) after simple attention.\n",
    "        # A standard LSTM layer expects 3D input (batch_size, timesteps, features).\n",
    "        # This part needs careful handling if attention is used before a second LSTM.\n",
    "        # For this iteration, if attention is used, we assume it's followed by Dense layers.\n",
    "        # So, if LSTM_UNITS_2 > 0 AND USE_ATTENTION, this architecture might need adjustment.\n",
    "        # Let's assume if USE_ATTENTION, it's the final recurrent/attention block before Dense.\n",
    "        if USE_ATTENTION:\n",
    "            print(\"Warning: Using Attention with a second LSTM layer requires careful architecture. Current setup assumes Attention is followed by Dense layers.\")\n",
    "        else: # No attention, standard stacked LSTM\n",
    "            x = LSTM(LSTM_UNITS_2, return_sequences=False)(x) # return_sequences=False for the last LSTM before Dense\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(DENSE_UNITS_1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    if DENSE_UNITS_2 > 0:\n",
    "        x = Dense(DENSE_UNITS_2, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', # Use this for integer targets\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Model built and compiled successfully.\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
